<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.PCA</name>
    </assembly>
    <members>
        <member name="M:Microsoft.ML.PcaCatalog.ProjectToPrincipalComponents(Microsoft.ML.Runtime.TransformsCatalog.ProjectionTransforms,System.String,System.String,System.String,System.Int32,System.Int32,System.Boolean,System.Nullable{System.Int32})">
            <summary>Initializes a new instance of <see cref="T:Microsoft.ML.Transforms.Projections.PrincipalComponentAnalysisEstimator"/>.</summary>
            <param name="catalog">The transform's catalog.</param>
            <param name="inputColumn">Input column to apply PrincipalComponentAnalysis on.</param>
            <param name="outputColumn">Optional output column. Null means <paramref name="inputColumn"/> is replaced.</param>
            <param name="weightColumn">The name of the weight column.</param>
            <param name="rank">The number of principal components.</param>
            <param name="overSampling">Oversampling parameter for randomized PrincipalComponentAnalysis training.</param>
            <param name="center">If enabled, data is centered to be zero mean.</param>
            <param name="seed">The seed for random number generation.</param>
        </member>
        <member name="M:Microsoft.ML.PcaCatalog.ProjectToPrincipalComponents(Microsoft.ML.Runtime.TransformsCatalog.ProjectionTransforms,Microsoft.ML.Transforms.Projections.PcaTransform.ColumnInfo[])">
            <summary>Initializes a new instance of <see cref="T:Microsoft.ML.Transforms.Projections.PrincipalComponentAnalysisEstimator"/>.</summary>
            <param name="catalog">The transform's catalog.</param>
            <param name="columns">Input columns to apply PrincipalComponentAnalysis on.</param>
        </member>
        <member name="T:Microsoft.ML.Trainers.PCA.RandomizedPcaTrainer">
            <summary>
            This trainer trains an approximate PCA using Randomized SVD algorithm
            Reference: https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf
            </summary>
            <remarks>
            This PCA can be made into Kernel PCA by using Random Fourier Features transform
            </remarks>
        </member>
        <member name="M:Microsoft.ML.Trainers.PCA.RandomizedPcaTrainer.#ctor(Microsoft.ML.Runtime.IHostEnvironment,System.String,System.String,System.Int32,System.Int32,System.Boolean,System.Nullable{System.Int32})">
            <summary>
            Initializes a new instance of <see cref="T:Microsoft.ML.Trainers.PCA.RandomizedPcaTrainer"/>.
            </summary>
            <param name="env">The local instance of the <see cref="T:Microsoft.ML.Runtime.IHostEnvironment"/>.</param>
            <param name="features">The name of the feature column.</param>
            <param name="weights">The name of the weight column.</param>
            <param name="rank">The number of components in the PCA.</param>
            <param name="oversampling">Oversampling parameter for randomized PCA training.</param>
            <param name="center">If enabled, data is centered to be zero mean.</param>
            <param name="seed">The seed for random number generation.</param>
        </member>
        <member name="M:Microsoft.ML.Trainers.PCA.RandomizedPcaTrainer.PostProcess(System.Single[][],System.Single[],System.Single[],System.Int32,System.Int32)">
            <summary>
            Modifies <paramref name="y"/> in place so it becomes <paramref name="y"/> * eigenvectors / eigenvalues.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Trainers.PCA.PcaPredictor">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
        </member>
        <member name="M:Microsoft.ML.Trainers.PCA.PcaPredictor.GetEigenVectors(Microsoft.ML.Runtime.Data.VBuffer{System.Single}[]@,System.Int32@)">
            <summary>
            Copies the top eigenvectors of the covariance matrix of the training data
            into a set of buffers.
            </summary>
            <param name="vectors">A possibly reusable set of vectors, which will
            be expanded as necessary to accomodate the data.</param>
            <param name="rank">Set to the rank, which is also the logical length
            of <paramref name="vectors"/>.</param>
        </member>
        <member name="M:Microsoft.ML.Trainers.PCA.PcaPredictor.GetMean(Microsoft.ML.Runtime.Data.VBuffer{System.Single}@)">
            <summary>
            Copies the mean vector of the training data.
            </summary>
        </member>
        <member name="T:Microsoft.ML.Transforms.Projections.PcaTransform">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
        </member>
        <member name="M:Microsoft.ML.Transforms.Projections.PcaTransform.ColumnInfo.#ctor(System.String,System.String,System.String,System.Int32,System.Int32,System.Boolean,System.Nullable{System.Int32})">
            <summary>
            Describes how the transformer handles one column pair.
            </summary>
            <param name="input">The column to apply PCA to.</param>
            <param name="output">The output column that contains PCA values.</param>
            <param name="weightColumn">The name of the weight column.</param>
            <param name="rank">The number of components in the PCA.</param>
            <param name="overSampling">Oversampling parameter for randomized PCA training.</param>
            <param name="center">If enabled, data is centered to be zero mean.</param>
            <param name="seed">The seed for random number generation.</param>
        </member>
        <member name="T:Microsoft.ML.Transforms.Projections.PrincipalComponentAnalysisEstimator">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
        </member>
        <member name="M:Microsoft.ML.Transforms.Projections.PrincipalComponentAnalysisEstimator.#ctor(Microsoft.ML.Runtime.IHostEnvironment,System.String,System.String,System.String,System.Int32,System.Int32,System.Boolean,System.Nullable{System.Int32})">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
            <param name="env">The environment to use.</param>
            <param name="inputColumn">Input column to project to Principal Component.</param>
            <param name="outputColumn">Output column. Null means <paramref name="inputColumn" /> is replaced.</param>
            <param name="weightColumn">The name of the weight column.</param>
            <param name="rank">The number of components in the PCA.</param>
            <param name="overSampling">Oversampling parameter for randomized PCA training.</param>
            <param name="center">If enabled, data is centered to be zero mean.</param>
            <param name="seed">The seed for random number generation.</param>
        </member>
        <member name="M:Microsoft.ML.Transforms.Projections.PrincipalComponentAnalysisEstimator.#ctor(Microsoft.ML.Runtime.IHostEnvironment,Microsoft.ML.Transforms.Projections.PcaTransform.ColumnInfo[])">
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
            <param name="env">The environment to use.</param>
            <param name="columns">The dataset columns to use, and their specific settings.</param>
        </member>
        <member name="M:Microsoft.ML.Transforms.Projections.PcaEstimatorExtensions.ToPrincipalComponents(Microsoft.ML.StaticPipe.Vector{System.Single},System.String,System.Int32,System.Int32,System.Boolean,System.Nullable{System.Int32})">
            <summary>
            Replaces the input vector with its projection to the principal component subspace,
            which can significantly reduce size of vector.
            </summary>
            <summary>
        PCA is a dimensionality-reduction transform which computes the projection of the feature vector onto a low-rank subspace. 
      </summary><remarks>
      <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principle Component Analysis (PCA)</a> is a dimensionality-reduction algorithm which computes the projection of the feature vector to onto a low-rank subspace.
      Its training is done using the technique described in the paper: <a href="https://arxiv.org/pdf/1310.6304v2.pdf">Combining Structured and Unstructured Randomness in Large Scale PCA</a>,
      and the paper <a href="https://arxiv.org/pdf/0909.4061v2.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
        <para>For more information, see also:</para>
        <list type="bullet">
          <item><description>
            <a href="https://web.stanford.edu/group/mmds/slides2010/Martinsson.pdf">Randomized Methods for Computing the Singular Value Decomposition (SVD) of very large matrices</a>
          </description></item>
          <item><description>
            <a href="https://arxiv.org/abs/0809.2274">A randomized algorithm for principal component analysis</a>
          </description></item>
          <item><description>
            <a href="http://users.cms.caltech.edu/~jtropp/papers/HMT11-Finding-Structure-SIREV.pdf">Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions</a>
          </description></item>
        </list>
      </remarks>
            <param name="input">The column to apply PCA to.</param>
            <param name="weightColumn">The name of the weight column.</param>
            <param name="rank">The number of components in the PCA.</param>
            <param name="overSampling">Oversampling parameter for randomized PCA training.</param>
            <param name="center">If enabled, data is centered to be zero mean.</param>
            <param name="seed">The seed for random number generation</param>
            <returns>Vector containing the principal components.</returns>
        </member>
    </members>
</doc>
